# -*- coding: utf-8 -*-
"""Cryto forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJ_GmEFP2E937YGtInRY5wNxv0ZwU5nK
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

raw = list(uploaded.keys())[0]

df = pd.read_csv(raw, skiprows=1)
df.head(20)

df['date'] = pd.to_datetime(df['date'])
df = df.sort_values('date').reset_index(drop=True)
df[['date']].head(), df[['date']].tail()

import numpy as np, pandas as pd, joblib
from sklearn.preprocessing import MinMaxScaler

FEATURE_COLS = ['open','high','low','close']
USE_VOLUME = True
VOLUME_COL = 'volume_btc'
ENCODER_LEN = 60

available = [c.lower() for c in df.columns]
print("Available top columns:", df.columns.tolist()[:8])


df.columns = [c.lower() for c in df.columns]

if USE_VOLUME and VOLUME_COL in df.columns:
    feature_cols = FEATURE_COLS + [VOLUME_COL]
else:
    feature_cols = FEATURE_COLS.copy()

# verify
for c in FEATURE_COLS:
    if c not in df.columns:
        raise RuntimeError(f"Required column '{c}' not found in df. Current cols: {df.columns.tolist()}")

print("Using feature columns:", feature_cols)

df = df.dropna(subset=FEATURE_COLS, how='any').reset_index(drop=True)
N = len(df)
print("Rows after dropna:", N)

train_cutoff = int(N * 0.8)
val_cutoff = int(N * 0.9)
print("train_cutoff:", train_cutoff, "val_cutoff:", val_cutoff, "N:", N)

scaler_feat = MinMaxScaler(feature_range=(0, 1))
scaler_feat.fit(df.loc[:train_cutoff-1, feature_cols].values.astype('float32'))
print("Fitted MinMaxScaler on train-only rows:", df.loc[:train_cutoff-1, feature_cols].shape)

values_scaled = scaler_feat.transform(df[feature_cols].values.astype('float32'))
print("values_scaled shape:", values_scaled.shape)

joblib.dump(scaler_feat, "scaler_feat_trainonly.pkl")
print("Saved scaler to 'scaler_feat_trainonly.pkl'")

scaled_sample_stats = pd.DataFrame({
    'col': feature_cols,
    'train_min': scaler_feat.data_min_,
    'train_max': scaler_feat.data_max_,
}).set_index('col')
print("\nTrain-fit scaler min/max per feature:")
print(scaled_sample_stats)

target_idx = feature_cols.index('close')
y_all_scaled = values_scaled[:, target_idx].astype('float32')

print("\nPreview (original -> scaled) first 5 rows:")
preview = pd.DataFrame(df[feature_cols].iloc[:5].values, columns=feature_cols)
preview_scaled = pd.DataFrame(values_scaled[:5,:], columns=feature_cols)
display(preview, preview_scaled)

np.save("values_scaled.npy", values_scaled)
print("Saved values_scaled.npy (you can reload with np.load).")

print("\nReady — feature scaling done, scaler saved. Next: train LSTM encoder on train-only windows (ENCODER_LEN = {})."
      .format(ENCODER_LEN))

# Block A: Train small LSTM encoder only on training windows (no leakage)
import numpy as np
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf

ENCODER_LEN = 60
EMBED_DIM = 8
BATCH_SIZE = 256
EPOCHS = 10
LSTM_UNITS = 64

assert 'values_scaled' in globals(), "values_scaled not found"
assert 'df' in globals(), "df not found"
N = len(values_scaled)
print("Rows:", N)

targets_raw = df['close'].values.astype('float32')

values_train_scaled = values_scaled[:train_cutoff]
targets_train = targets_raw[:train_cutoff]

gen_train = TimeseriesGenerator(values_train_scaled, targets_train, length=ENCODER_LEN, batch_size=BATCH_SIZE)
print("Train windows (batches):", len(gen_train))

timesteps = ENCODER_LEN
features = values_scaled.shape[1]

inp = Input(shape=(timesteps, features), name='encoder_input')
x = LSTM(LSTM_UNITS, return_sequences=False, name='encoder_lstm')(inp)
x = Dropout(0.2, name='encoder_dropout')(x)
emb = Dense(EMBED_DIM, activation='relu', name='lstm_embedding')(x)
pred = Dense(1, activation='linear', name='next_close')(emb)

train_model = Model(inputs=inp, outputs=pred)
encoder_model = Model(inputs=inp, outputs=emb)

train_model.compile(optimizer='adam', loss='mse')
train_model.summary()

es = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1)
ckpt = ModelCheckpoint("lstm_train_model_trainonly.h5", monitor='val_loss', save_best_only=True, verbose=1)

val_ratio = 0.1
steps = len(gen_train)
if steps < 10:

    history = train_model.fit(gen_train, epochs=EPOCHS, callbacks=[es, ckpt], verbose=1)
else:

    train_steps = int((1.0 - val_ratio) * steps)
    val_steps = max(1, steps - train_steps)
    history = train_model.fit(gen_train, epochs=EPOCHS,
                              steps_per_epoch=max(1, train_steps),
                              validation_data=gen_train,
                              validation_steps=val_steps,
                              callbacks=[es, ckpt],
                              verbose=1)

encoder_model.save("lstm_encoder_model_trainonly.h5")
print("Saved lstm_train_model_trainonly.h5 and lstm_encoder_model_trainonly.h5")

# >>> Single-step: STREAM EMBEDDINGS (run exactly this cell) <<<

import os, gc
import numpy as np, pandas as pd
from tensorflow.keras.models import load_model
from tqdm import tqdm

ENCODER_LEN = 60
EMBED_DIM = 8
BATCH_SIZE = 512
OUT_EMB_FILE = "lstm_embeddings_stream.csv"

encoder_model = load_model("lstm_encoder_model_trainonly.h5")
print("Loaded encoder:", "lstm_encoder_model_trainonly.h5")

if 'values_scaled' not in globals():
    values_scaled = np.load("values_scaled.npy")
print("values_scaled shape:", values_scaled.shape)

N = len(values_scaled)
first_idx = ENCODER_LEN
expected_windows = N - ENCODER_LEN
print("Expected windows:", expected_windows)

start_idx = first_idx
if os.path.exists(OUT_EMB_FILE):
    try:
        last_line = !tail -n 1 {OUT_EMB_FILE}
        last_line = last_line[0]
        last_idx = int(last_line.split(",")[0])
        start_idx = last_idx + 1
        print("Resuming at index:", start_idx)
    except Exception as e:
        print("Couldn't parse existing file, will overwrite. Error:", e)
        os.remove(OUT_EMB_FILE)
        start_idx = first_idx

def make_batch_windows(arr, start_i, batch_count, seq_len):
    idxs = np.arange(start_i - seq_len, start_i - seq_len + batch_count)[:, None] + np.arange(seq_len)
    return arr[idxs]

mode = "a" if (os.path.exists(OUT_EMB_FILE) and start_idx > first_idx) else "w"
if mode == "w":
    with open(OUT_EMB_FILE, "w") as f:
        f.write("index," + ",".join([f"lstm_emb_{i}" for i in range(EMBED_DIM)]) + "\n")

written = start_idx - first_idx
i = start_idx
pbar = tqdm(total=expected_windows, initial=written, unit="embeddings")
while i < N:
    batch_count = min(BATCH_SIZE, N - i)
    if batch_count <= 0:
        break
    batch_windows = make_batch_windows(values_scaled, i, batch_count, ENCODER_LEN)
    emb_batch = encoder_model.predict(batch_windows, batch_size=min(128, batch_count), verbose=0)
    rows = []
    for k in range(emb_batch.shape[0]):
        idx = i + k
        vals = emb_batch[k].astype(float)
        rows.append(f"{idx}," + ",".join([f"{v:.8f}" for v in vals]) + "\n")
    with open(OUT_EMB_FILE, "a") as f:
        f.writelines(rows)
    written += emb_batch.shape[0]
    pbar.update(emb_batch.shape[0])
    i += batch_count
    del batch_windows, emb_batch, rows
    gc.collect()
pbar.close()
print("Done. Embeddings written to:", OUT_EMB_FILE)

import pandas as pd
import numpy as np

if "df" not in globals():
    df = pd.read_csv("btcusd_1-min_data.csv")

print("Original df shape:", df.shape)

emb = pd.read_csv("lstm_embeddings_stream.csv")
print("Embeddings loaded:", emb.shape)

emb_start = 60

emb = emb.rename(columns={"index": "df_index"})
emb = emb.set_index("df_index")

df = df.iloc[emb_start:].reset_index(drop=True)
emb = emb.iloc[emb_start:].reset_index(drop=True)

print("Aligned df:", df.shape)
print("Aligned embeddings:", emb.shape)

df_with_emb = pd.concat([df, emb], axis=1)

print("Merged df_with_emb:", df_with_emb.shape)
df_with_emb.head()

import pandas as pd
import numpy as np

df_hybrid = df_with_emb.copy()

print("Starting df_hybrid shape:", df_hybrid.shape)

# --- 1) Ensure date is datetime ---
df_hybrid['date'] = pd.to_datetime(df_hybrid['date'])

# --- 2) Add time features (important for TFT) ---
df_hybrid['hour'] = df_hybrid['date'].dt.hour
df_hybrid['day'] = df_hybrid['date'].dt.day
df_hybrid['month'] = df_hybrid['date'].dt.month
df_hybrid['weekday'] = df_hybrid['date'].dt.weekday

# --- 3) Define target ---
TARGET_COL = "close"

# --- 4) Add ID column (TFT requires a group id) ---
df_hybrid['series_id'] = 0  # since it's one long series

print("Processed df_hybrid shape:", df_hybrid.shape)
df_hybrid.head()

import torch
from torch.utils.data import Dataset, DataLoader

# CONFIG
ENCODER_LEN = 60     # past window
PRED_LEN = 1         # predict next close price

TARGET_COL = "close"

# Columns for model (inputs)
feature_cols = [
    "open", "high", "low", "close",
    "hour", "day", "month", "weekday",
] + [f"lstm_emb_{i}" for i in range(8)]

print("Total input features:", len(feature_cols))


# ---------------------------------------------
# Custom PyTorch Dataset for TFT
# ---------------------------------------------
class CryptoDataset(Dataset):
    def __init__(self, df, encoder_len=60, pred_len=1):
        self.df = df
        self.encoder_len = encoder_len
        self.pred_len = pred_len

        self.X = df[feature_cols].values.astype("float32")
        self.y = df[TARGET_COL].values.astype("float32")

        self.max_idx = len(df) - pred_len

    def __len__(self):
        return self.max_idx - self.encoder_len

    def __getitem__(self, idx):
        start = idx
        end = idx + self.encoder_len

        x_enc = self.X[start:end]
        y_future = self.y[end:end+self.pred_len]

        return torch.tensor(x_enc), torch.tensor(y_future)


# ---------------------------------------------
# Split data indices
# ---------------------------------------------
N = len(df_hybrid)
train_end = int(N * 0.8)
val_end = int(N * 0.9)

train_df = df_hybrid.iloc[:train_end]
val_df   = df_hybrid.iloc[train_end:val_end]
test_df  = df_hybrid.iloc[val_end:]

print("Train:", train_df.shape)
print("Val:", val_df.shape)
print("Test:", test_df.shape)


# ---------------------------------------------
# Build datasets
# ---------------------------------------------
train_ds = CryptoDataset(train_df, ENCODER_LEN, PRED_LEN)
val_ds   = CryptoDataset(val_df, ENCODER_LEN, PRED_LEN)
test_ds  = CryptoDataset(test_df, ENCODER_LEN, PRED_LEN)

print("train_ds len:", len(train_ds))
print("val_ds len:", len(val_ds))
print("test_ds len:", len(test_ds))


# ---------------------------------------------
# DataLoaders
# ---------------------------------------------
train_loader = DataLoader(train_ds, batch_size=512, shuffle=True, drop_last=True)
val_loader   = DataLoader(val_ds, batch_size=512, shuffle=False, drop_last=True)
test_loader  = DataLoader(test_ds, batch_size=512, shuffle=False, drop_last=True)

print("Dataloaders ready!")

import torch
import torch.nn as nn
import torch.nn.functional as F


# -------------------------
# GRN (Gated Residual Network)
# -------------------------
class GRN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, input_size)
        self.elu = nn.ELU()
        self.gate = nn.Linear(input_size, input_size)
        self.norm = nn.LayerNorm(input_size)

    def forward(self, x):
        residual = x
        x = self.elu(self.fc1(x))
        x = self.fc2(x)
        gate = torch.sigmoid(self.gate(residual))
        x = gate * x + (1 - gate) * residual
        return self.norm(x)


# -------------------------
# Variable Selection Network
# -------------------------
class VSN(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.fc_weights = nn.Linear(input_dim, input_dim)
        self.feature_grns = nn.ModuleList([GRN(1, hidden_dim) for _ in range(input_dim)])

    def forward(self, x):
        # x: (batch, seq, features)
        weights = torch.softmax(self.fc_weights(x.mean(dim=1)), dim=-1)
        outputs = []
        for i, grn in enumerate(self.feature_grns):
            feat = x[:, :, i:i+1]
            outputs.append(grn(feat))
        outputs = torch.stack(outputs, dim=-1)  # (B, T, 1, F)
        outputs = outputs.squeeze(2)
        out = torch.sum(outputs * weights.unsqueeze(1), dim=-1)
        return out


# -------------------------
# Temporal Fusion Transformer (Simplified)
# -------------------------
class TFTModel(nn.Module):
    def __init__(self, feature_dim, hidden_dim=64, num_heads=4, dropout=0.1):
        super().__init__()
        self.feature_dim = feature_dim

        # Variable selection
        self.vsn = VSN(feature_dim, hidden_dim)

        # LSTM encoder
        self.encoder_lstm = nn.LSTM(
            input_size=feature_dim,
            hidden_size=hidden_dim,
            batch_first=True
        )

        # Multi-head attention
        self.attn = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        # GRN for post-attention fusion
        self.grn_fusion = GRN(hidden_dim, hidden_dim)

        # Final regression head
        self.fc_out = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        # 1) Variable Selection
        x = self.vsn(x)  # (B, T)

        # reshape for LSTM
        x = x.unsqueeze(-1)  # (B, T, 1) – but we need feature_dim?
        x = x.repeat(1, 1, self.feature_dim)  # expand to expected feature dim

        # 2) Encoder LSTM
        enc_out, _ = self.encoder_lstm(x)  # (B, T, H)

        # 3) Attention
        attn_out, _ = self.attn(enc_out, enc_out, enc_out)

        # 4) Fusion
        fused = self.grn_fusion(attn_out)

        # 5) Final step prediction (use last timestep)
        last = fused[:, -1, :]
        out = self.fc_out(last)

        return out

# >>> Single step: define the TFTModel wrapper, instantiate and move to device <<<

import torch
import torch.nn as nn

# Lightweight TFT wrapper using the GRN and VSN you already defined
class TFTModel(nn.Module):
    def __init__(self, feature_dim, hidden_dim=64, num_heads=4, dropout=0.1):
        super().__init__()
        self.feature_dim = feature_dim
        self.hidden_dim = hidden_dim

        # variable selection
        self.vsn = VSN(feature_dim, hidden_dim)

        # encoder LSTM
        self.encoder_lstm = nn.LSTM(
            input_size=feature_dim,
            hidden_size=hidden_dim,
            batch_first=True
        )

        # multi-head attention
        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True, dropout=dropout)

        # GRN fusion
        self.grn_fusion = GRN(hidden_dim, hidden_dim)

        # final head
        self.fc_out = nn.Sequential(
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x):
        # x: (B, T, F)
        # 1) variable selection -> returns (B, T)
        vsn_out = self.vsn(x)                      # (B, T)
        # expand back to per-feature shape for LSTM (simple repeat)
        x_rep = vsn_out.unsqueeze(-1).repeat(1, 1, self.feature_dim)  # (B, T, F)

        # 2) LSTM encoder
        enc_out, _ = self.encoder_lstm(x_rep)      # (B, T, H)

        # 3) self-attention
        attn_out, _ = self.attn(enc_out, enc_out, enc_out)  # (B, T, H)

        # 4) fusion
        fused = self.grn_fusion(attn_out)          # (B, T, H)

        # 5) prediction from last timestep
        last = fused[:, -1, :]
        out = self.fc_out(last).squeeze(-1)        # (B,)
        return out

# instantiate model (use the feature dim you printed earlier)
feature_dim = 16   # <- this should match "Total input features" you saw earlier
model = TFTModel(feature_dim=feature_dim, hidden_dim=64, num_heads=4, dropout=0.1)

# move to device and print info
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# count params
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print("Model instantiated.")
print("Device:", device)
print("Total params:", total_params)
print("Trainable params:", trainable_params)

# quick forward test with a dummy batch to ensure shapes are OK
with torch.no_grad():
    x_dummy = torch.randn(2, 60, feature_dim).to(device)   # batch=2, seq=60
    y_dummy = model(x_dummy)
print("Forward OK — output shape:", y_dummy.shape)

# ---------------- Single step: training loop for TFT hybrid ----------------
import torch, time, math, os, numpy as np
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import mean_squared_error

# CONFIG
EPOCHS = 20
LR = 1e-3
BATCH_CLIP = 0.5
PATIENCE = 4
BEST_PATH = "tft_hybrid_best.pth"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

assert 'model' in globals(), "model not found — run model cell first"
assert 'train_loader' in globals() and 'val_loader' in globals(), "dataloaders missing"

model = model.to(device)

scaler_feat_exists = 'scaler_feat' in globals()
emb_scaler_exists = 'emb_scaler' in globals()
print("scaler_feat_exists:", scaler_feat_exists, "emb_scaler_exists:", emb_scaler_exists)

if 'feature_cols' not in globals():
    raise RuntimeError("feature_cols missing")

# determine ohlc indices
try:
    scaler_input_count = scaler_feat.n_features_in_
except:
    scaler_input_count = 4

ohlc_indices = list(range(scaler_input_count))
emb_indices = [i for i,n in enumerate(feature_cols) if "lstm_emb_" in str(n)]
print("OHLC indices:", ohlc_indices, "Emb indices:", emb_indices)

try:
    target_idx_in_feat = feature_cols.index("close")
except:
    target_idx_in_feat = None


def inverse_close_array(vals):
    if not scaler_feat_exists:
        return np.array(vals)
    M = len(vals)
    temp = np.zeros((M, scaler_input_count), dtype="float32")
    close_pos = target_idx_in_feat if target_idx_in_feat is not None else 3
    temp[:, close_pos] = vals
    inv = scaler_feat.inverse_transform(temp)
    return inv[:, close_pos]


optimizer = optim.Adam(model.parameters(), lr=LR)

# FIXED: remove verbose=True
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=2
)

criterion = nn.MSELoss()

best_val = float('inf')
patience_ctr = 0


# ---------------- TRAIN LOOP ----------------
for epoch in range(1, EPOCHS+1):
    t0 = time.time()
    model.train()
    train_losses = []

    for xb, yb in train_loader:
        xb_np = xb.numpy()

        # scale OHLC
        if scaler_feat_exists:
            B, T, F = xb_np.shape
            flat = xb_np[:, :, ohlc_indices].reshape(-1, len(ohlc_indices))
            xb_np[:, :, ohlc_indices] = scaler_feat.transform(flat).reshape(B, T, len(ohlc_indices))

        # scale embeddings (if scaler exists — currently False)
        if emb_scaler_exists and len(emb_indices) > 0:
            B, T, F = xb_np.shape
            flat = xb_np[:, :, emb_indices].reshape(-1, len(emb_indices))
            xb_np[:, :, emb_indices] = emb_scaler.transform(flat).reshape(B, T, len(emb_indices))

        xb_t = torch.tensor(xb_np, dtype=torch.float32, device=device)
        yb_t = yb.to(device).float().view(-1)

        optimizer.zero_grad()
        pred = model(xb_t)
        loss = criterion(pred, yb_t)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), BATCH_CLIP)
        optimizer.step()

        train_losses.append(loss.item())

    avg_train = np.mean(train_losses)


    # ---------------- VAL ----------------
    model.eval()
    val_losses = []
    preds_all = []
    trues_all = []

    with torch.no_grad():
        for xb, yb in val_loader:
            xb_np = xb.numpy()

            if scaler_feat_exists:
                B, T, F = xb_np.shape
                flat = xb_np[:, :, ohlc_indices].reshape(-1, len(ohlc_indices))
                xb_np[:, :, ohlc_indices] = scaler_feat.transform(flat).reshape(B, T, len(ohlc_indices))

            xb_t = torch.tensor(xb_np, dtype=torch.float32, device=device)
            yb_t = yb.to(device).float().view(-1)

            pred = model(xb_t)
            loss = criterion(pred, yb_t)
            val_losses.append(loss.item())

            preds_all.append(pred.cpu().numpy())
            trues_all.append(yb_t.cpu().numpy())

    avg_val = np.mean(val_losses)

    y_pred_scaled  = np.concatenate(preds_all)
    y_true_scaled  = np.concatenate(trues_all)
    y_pred_price   = inverse_close_array(y_pred_scaled)
    y_true_price   = inverse_close_array(y_true_scaled)
    rmse_price     = math.sqrt(mean_squared_error(y_true_price, y_pred_price))


    scheduler.step(avg_val)

    print(f"Epoch {epoch:02d} | train {avg_train:.4e} | val {avg_val:.4e} | RMSE_price {rmse_price:.3f} | time {time.time()-t0:.1f}s")

    if avg_val < best_val:
        best_val = avg_val
        patience_ctr = 0
        torch.save(model.state_dict(), BEST_PATH)
        print("  -> New BEST model saved")
    else:
        patience_ctr += 1
        print(f"  patience {patience_ctr}/{PATIENCE}")
        if patience_ctr >= PATIENCE:
            print("EARLY STOPPING TRIGGERED")
            break


# Load best model
if os.path.exists(BEST_PATH):
    model.load_state_dict(torch.load(BEST_PATH, map_location=device))
    print("Loaded best model:", BEST_PATH)

import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

# ------------------------------
# Load best model
# ------------------------------
model.load_state_dict(torch.load("tft_hybrid_best.pth"))
model.eval()

print("Loaded best model!")

# ------------------------------
# Run on test set
# ------------------------------
all_preds = []
all_true  = []

with torch.no_grad():
    for xb, yb in tqdm(test_loader):
        xb = xb.to(device)
        preds = model(xb).cpu().numpy().flatten()
        all_preds.append(preds)
        all_true.append(yb.numpy().flatten())

# convert to arrays
all_preds = np.concatenate(all_preds)
all_true  = np.concatenate(all_true)

# ------------------------------
# RMSE on test set
# ------------------------------
rmse = np.sqrt(np.mean((all_preds - all_true)**2))
print("\n==============================")
print(f" Test RMSE (price units): {rmse:,.4f}")
print("==============================")

# ------------------------------
# Direction Accuracy
# ------------------------------
true_diff = np.sign(np.diff(all_true))
pred_diff = np.sign(np.diff(all_preds))
direction_acc = (true_diff == pred_diff).mean() * 100

print(f" Direction Accuracy: {direction_acc:.2f}%")

# ------------------------------
# Save predictions for report
# ------------------------------
np.save("tft_true.npy", all_true)
np.save("tft_pred.npy", all_preds)

print("\nSaved: tft_true.npy and tft_pred.npy")

# ONE CELL: load or recompute predictions, diagnose NaNs, compute safe metrics, map bad indices
import os, numpy as np, math
from sklearn.metrics import mean_squared_error

# helper: try several saved filenames
candidates = [
    ("preds_returns_price.npy", "true_returns_price.npy"),
    ("tft_pred.npy", "tft_true.npy"),
    ("tft_pred_safe.npy", "tft_true_safe.npy"),
    ("preds.npy", "true.npy")
]

preds_file = None
true_file = None
for p,t in candidates:
    if os.path.exists(p) and os.path.exists(t):
        preds_file, true_file = p, t
        break

# if saved files exist, load them; otherwise re-run inference to produce them
if preds_file is not None:
    preds_price = np.load(preds_file)
    true_price = np.load(true_file)
    print(f"Loaded saved arrays: {preds_file}, {true_file}")
else:
    print("Saved pred/true files not found — recomputing predictions from model on test_loader.")
    # sanity checks
    assert 'model' in globals(), "model not present in environment."
    assert 'test_loader' in globals(), "test_loader not present in environment."
    # run inference
    model.eval()
    preds_list = []
    true_list = []
    with torch.no_grad():
        for xb, yb in test_loader:
            xb = xb.to(device)
            preds = model(xb).cpu().numpy().flatten()
            preds_list.append(preds)
            true_list.append(yb.numpy().flatten())
    preds_price = np.concatenate(preds_list)
    true_price = np.concatenate(true_list)
    # save for convenience
    np.save("recomputed_preds.npy", preds_price)
    np.save("recomputed_true.npy", true_price)
    print("Saved recomputed_preds.npy and recomputed_true.npy")

# report shapes
print("Shapes -> preds:", preds_price.shape, " true:", true_price.shape)

# detect NaNs / infs
is_finite_pred = np.isfinite(preds_price)
is_finite_true = np.isfinite(true_price)
both_finite = is_finite_pred & is_finite_true

n_total = len(preds_price)
n_bad_pred = np.sum(~is_finite_pred)
n_bad_true = np.sum(~is_finite_true)
n_bad_both = np.sum(~both_finite)

print(f"Total samples: {n_total}")
print(f"Bad preds (NaN/Inf): {n_bad_pred}")
print(f"Bad trues (NaN/Inf): {n_bad_true}")
print(f"Pairs with any bad value: {n_bad_both}")

# show first bad indices
if n_bad_both > 0:
    bad_idx = np.where(~both_finite)[0]
    print("First bad indices (0-based in pred arrays):", bad_idx[:20])

    # Map bad indices back to df_hybrid rows (approx)
    if 'val_end' in globals() and 'ENCODER_LEN' in globals():
        # recall: test_df = df_hybrid.iloc[val_end:].reset_index(drop=True)
        # prediction for dataset index idx corresponds to df_hybrid index = val_end + ENCODER_LEN + idx
        print("\nMapping pred-array index -> approx df_hybrid index (first 10):")
        for i in bad_idx[:10]:
            approx_df_index = val_end + ENCODER_LEN + i
            print(f"pred_idx {i} -> df_hybrid index approx {approx_df_index}")
        # show those rows for inspection if df_hybrid present
        if 'df_hybrid' in globals():
            sample_rows = [val_end + ENCODER_LEN + int(i) for i in bad_idx[:5]]
            print("\nPreview of corresponding df_hybrid rows (first 5 bad):")
            display(df_hybrid.iloc[sample_rows])
    else:
        print("val_end or ENCODER_LEN not found — cannot map automatically.")

# compute safe metrics ignoring NaNs
valid_mask = both_finite
if np.sum(valid_mask) == 0:
    raise RuntimeError("No valid prediction pairs after removing NaNs — need to debug source data.")

preds_ok = preds_price[valid_mask]
true_ok  = true_price[valid_mask]

rmse = math.sqrt(mean_squared_error(true_ok, preds_ok))
dir_acc = (np.sign(np.diff(true_ok)) == np.sign(np.diff(preds_ok))).mean() * 100

print("\n=== SAFE METRICS (ignoring NaNs) ===")
print(f"Valid sample count: {preds_ok.shape[0]} / {n_total}")
print(f"Test RMSE (price units): {rmse:,.4f}")
print(f"Directional accuracy (%): {dir_acc:.2f}%")

# Save cleaned arrays
np.save("tft_pred_safe.npy", preds_ok)
np.save("tft_true_safe.npy", true_ok)
print("Saved cleaned arrays: tft_pred_safe.npy, tft_true_safe.npy")

# If many NaNs found in true_price, suggest quick fix
if n_bad_true > 0:
    print("\nNOTE: There are NaNs in true_price (target). This usually means df_hybrid has missing 'close' values for some rows.")
    print("Quick fix (not auto-applied): forward-fill 'close' and embedding columns in df_hybrid, then re-create datasets and re-run evaluation.")
    print("Run this snippet to apply the fix and then re-run the returns-evaluation cell:")
    print('''\n# Quick fix snippet - run if you decide to repair df_hybrid\nfor col in ['close'] + [c for c in df_hybrid.columns if str(c).startswith('lstm_emb_')]:\n    df_hybrid[col] = df_hybrid[col].fillna(method='ffill').fillna(method='bfill')\nprint('Applied forward/back fill to close & embeddings.')\n''')

# METRICS & SUMMARY
import os, numpy as np, math, pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error

# candidate files (order matters) - adjust if your filenames differ
candidates = [
    ("preds_scaledemb_returns_price.npy", "true_scaledemb_returns_price.npy"),
    ("preds_returns_price.npy", "true_returns_price.npy"),
    ("preds.npy","true.npy"),
    ("tft_pred.npy","tft_true.npy"),
    ("recovered_preds.npy","recovered_true.npy"),
    ("preds_scaledemb_returns_price.npy","true_scaledemb_returns_price.npy"),
    ("preds_returns_price.npy","true_returns_price.npy"),
]

pred_file = true_file = None
for p,t in candidates:
    if os.path.exists(p) and os.path.exists(t):
        pred_file, true_file = p, t
        break

# helper to recompute if nothing found (fast, but will require model + test_loader in memory)
if pred_file is None:
    print("No saved pred/true files found; attempting to recompute from model & test_loader.")
    assert 'model' in globals() and 'test_loader' in globals(), "No saved files AND model/test_loader not in memory."
    preds = []
    trues = []
    with torch.no_grad():
        for xb, yb, lastp in test_loader:
            xb = xb.to(device)
            p_rel = model(xb).cpu().numpy().flatten()
            lastp_np = lastp.numpy().flatten()
            p_price = lastp_np * (1.0 + p_rel)
            t_price = lastp_np * (1.0 + yb.numpy().flatten())
            preds.append(p_price); trues.append(t_price)
    preds = np.concatenate(preds); trues = np.concatenate(trues)
    np.save("recomputed_preds.npy", preds); np.save("recomputed_true.npy", trues)
    pred_file, true_file = "recomputed_preds.npy", "recomputed_true.npy"
    print("Recomputed and saved:", pred_file, true_file)
else:
    print("Using saved files:", pred_file, true_file)
    preds = np.load(pred_file)
    trues = np.load(true_file)

# basic checks
assert preds.shape == trues.shape, f"Shape mismatch: preds {preds.shape} vs trues {trues.shape}"

# compute metrics (price units)
mse = mean_squared_error(trues, preds)
rmse = math.sqrt(mse)
mae = mean_absolute_error(trues, preds)
# MAPE: avoid div by zero
nonzero = (np.abs(trues) > 1e-9)
mape = (np.abs((trues[nonzero] - preds[nonzero]) / trues[nonzero])).mean() * 100 if nonzero.any() else float('nan')

# directional accuracy
true_dir = np.sign(np.diff(trues))
pred_dir = np.sign(np.diff(preds))
direction_acc = float((true_dir == pred_dir).mean()) * 100

# up/down confusion
# map -1,0,1 into up/down (treat 0 as no-change -> count as correct only if both 0)
def dir_label(arr):
    lab = np.sign(arr)
    lab[lab>0] = 1
    lab[lab<0] = -1
    lab[lab==0] = 0
    return lab
t_lab = dir_label(np.diff(trues))
p_lab = dir_label(np.diff(preds))

# confusion matrix counts
tp_up = ((t_lab==1) & (p_lab==1)).sum()
tp_down = ((t_lab==-1) & (p_lab==-1)).sum()
pred_up = (p_lab==1).sum()
pred_down = (p_lab==-1).sum()
true_up = (t_lab==1).sum()
true_down = (t_lab==-1).sum()
no_change = ((t_lab==0) & (p_lab==0)).sum()

# summary dataframe
summary = {
    "samples": int(len(preds)),
    "rmse_price": float(rmse),
    "mae_price": float(mae),
    "mape_percent": float(mape),
    "direction_acc_percent": float(direction_acc),
    "true_up": int(true_up),
    "true_down": int(true_down),
    "pred_up": int(pred_up),
    "pred_down": int(pred_down),
    "tp_up": int(tp_up),
    "tp_down": int(tp_down),
    "no_change_matches": int(no_change)
}

print("\n=== METRICS SUMMARY ===")
for k,v in summary.items():
    print(f"{k:20s}: {v}")
print("========================\n")

# save metrics to CSV for report
metrics_df = pd.DataFrame([summary])
metrics_df.to_csv("tft_metrics_summary.csv", index=False)
print("Saved metrics to tft_metrics_summary.csv")

# also save preds+trues to a CSV (first N rows to keep file small); save full as numpy already done
Nsave = min(200000, len(preds))
out_df = pd.DataFrame({"true_price": trues[:Nsave], "pred_price": preds[:Nsave]})
out_df.to_csv("tft_preds_sample.csv", index=False)
print("Saved sample preds CSV:", "tft_preds_sample.csv")

# VISUALS: plots (prediction vs true, error histogram, rolling RMSE, scatter)
import matplotlib.pyplot as plt
import numpy as np, pandas as pd
from scipy.stats import gaussian_kde

# load the safe arrays used above (preds, trues)
# If not in memory, this loads from files used in previous cell
try:
    preds, trues
except NameError:
    preds = np.load(pred_file)
    trues = np.load(true_file)

# 1) Pred vs True (first N)
N = 300  # first N points to display
plt.figure(figsize=(12,4))
plt.plot(trues[:N], label="true")
plt.plot(preds[:N], label="pred")
plt.title(f"Pred vs True (first {N})")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 2) Scatter true vs pred (sample for speed)
S = min(10000, len(preds))
idx = np.linspace(0, len(preds)-1, S, dtype=int)
plt.figure(figsize=(5,5))
plt.scatter(trues[idx], preds[idx], s=6, alpha=0.3)
mn = min(trues[idx].min(), preds[idx].min()); mx = max(trues[idx].max(), preds[idx].max())
plt.plot([mn,mx],[mn,mx], linestyle='--')
plt.xlabel("True price"); plt.ylabel("Pred price")
plt.title(f"True vs Pred (sample {S})")
plt.grid(True)
plt.tight_layout()
plt.show()

# 3) Error histogram (absolute error) with KDE overlay
errors = preds - trues
abs_err = np.abs(errors)
plt.figure(figsize=(8,4))
plt.hist(abs_err, bins=200, density=True, alpha=0.6)
try:
    kde = gaussian_kde(abs_err)
    xs = np.linspace(abs_err.min(), abs_err.max(), 200)
    plt.plot(xs, kde(xs), linewidth=2)
except Exception:
    pass
plt.title("Absolute Error Distribution")
plt.xlabel("Absolute error (price units)")
plt.tight_layout()
plt.show()

# 4) Rolling RMSE over time (window in samples)
window = 1000
abs_sq = (errors**2)
# compute rolling rmse via convolution (fast)
kernel = np.ones(window)
cum = np.convolve(abs_sq, kernel, mode='valid')
rmse_roll = np.sqrt(cum / window)
plt.figure(figsize=(12,4))
plt.plot(rmse_roll)
plt.title(f"Rolling RMSE (window={window})")
plt.xlabel("sample index (rolling)")
plt.tight_layout()
plt.show()

# 5) Direction agreement timeline (boolean)
direction_true = np.sign(np.diff(trues))
direction_pred = np.sign(np.diff(preds))
agree = (direction_true == direction_pred).astype(int)
plt.figure(figsize=(12,2))
plt.plot(agree[:2000], linewidth=0.8)
plt.title("Directional agreement (1=agree, 0=disagree) — first 2000 steps")
plt.ylim(-0.1,1.1)
plt.tight_layout()
plt.show()

# 6) Save figure files (png)
plt.figure(figsize=(6,3)); plt.plot(trues[:N], label="true"); plt.plot(preds[:N], label="pred"); plt.legend(); plt.tight_layout(); plt.savefig("pred_vs_true_sample.png"); plt.close()
plt.figure(figsize=(5,5)); plt.scatter(trues[idx], preds[idx], s=6, alpha=0.3); plt.plot([mn,mx],[mn,mx], linestyle='--'); plt.tight_layout(); plt.savefig("true_vs_pred_scatter.png"); plt.close()
print("Saved PNGs: pred_vs_true_sample.png, true_vs_pred_scatter.png")